{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","machine_shape":"hm","collapsed_sections":["omCK664qOXNc","_TJtDtiTPVQg"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"omCK664qOXNc"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prp6R7BY60Y7","executionInfo":{"status":"ok","timestamp":1694938873141,"user_tz":-120,"elapsed":17627,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}},"outputId":"0c1836ba-785a-48d2-83e8-afe9a9fd00bc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/SebastianoBarresi/DNLP_project_2023.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gch2uRAUzBeM","executionInfo":{"status":"ok","timestamp":1694938874362,"user_tz":-120,"elapsed":1225,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}},"outputId":"c9e4960f-2de6-4877-8a26-562c8b45693e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'DNLP_project_2023'...\n","remote: Enumerating objects: 38, done.\u001b[K\n","remote: Counting objects: 100% (38/38), done.\u001b[K\n","remote: Compressing objects: 100% (28/28), done.\u001b[K\n","remote: Total 38 (delta 12), reused 31 (delta 8), pack-reused 0\u001b[K\n","Receiving objects: 100% (38/38), 2.56 MiB | 19.41 MiB/s, done.\n","Resolving deltas: 100% (12/12), done.\n"]}]},{"cell_type":"code","source":["%cd /content/DNLP_project_2023"],"metadata":{"id":"-Hxd0flNzHvS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694938874363,"user_tz":-120,"elapsed":12,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}},"outputId":"53058b13-d13b-4e9b-c842-52b33a4d45e1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/DNLP_project_2023\n"]}]},{"cell_type":"code","source":["!pip install pip install torch==1.11.0+cu102 --extra-index-url https://download.pytorch.org/whl/cu102\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install rouge\n","!pip install wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FX4gCLeWzLVM","executionInfo":{"status":"ok","timestamp":1694938966221,"user_tz":-120,"elapsed":91866,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}},"outputId":"3f24103c-5ad3-4947-85db-110ec911fd23"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu102\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n","Collecting install\n","  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n","Collecting torch==1.11.0+cu102\n","  Downloading https://download.pytorch.org/whl/cu102/torch-1.11.0%2Bcu102-cp310-cp310-linux_x86_64.whl (750.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu102) (4.5.0)\n","Installing collected packages: torch, install\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.1+cu118\n","    Uninstalling torch-2.0.1+cu118:\n","      Successfully uninstalled torch-2.0.1+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.11.0+cu102 which is incompatible.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.11.0+cu102 which is incompatible.\n","torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.11.0+cu102 which is incompatible.\n","torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.11.0+cu102 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed install-1.3.5 torch-1.11.0+cu102\n","Collecting transformers\n","  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n","Collecting wandb\n","  Downloading wandb-0.15.10-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.36-py3-none-any.whl (189 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Collecting pathtools (from wandb)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=3823336908d5a08fdfcb7b1ac40210ed21daafaa18200e51f1247a7095158ded\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built pathtools\n","Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n","Successfully installed GitPython-3.1.36 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.31.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.10\n"]}]},{"cell_type":"markdown","source":["## TEAM"],"metadata":{"id":"_TJtDtiTPVQg"}},{"cell_type":"markdown","source":["### Roberta Large\n"],"metadata":{"id":"e25lSTsmdEUm"}},{"cell_type":"code","source":["!python train_qasc.py --name \"roberta-large\" --epochs 5 --lr 1e-6 --bs 40 --eval-bs 40 --savings_path /content/drive/MyDrive/Project/Saves"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694896558058,"user_tz":-120,"elapsed":1734882,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}},"outputId":"8e61f532-452a-4e2f-ddcf-9434a3a897aa","id":"ThhEBi8udEUq"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-09-16 20:07:06.320020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Namespace(lr=1e-06, wd=0.0, warm_up_steps=0, adam_epsilon=1e-08, bs=40, eval_bs=40, epochs=5, name='roberta-large', shuffle=False, input_format='1', savings_path='/content/drive/MyDrive/Project/Saves', seed=0)\n","Downloading (…)lve/main/config.json: 100% 482/482 [00:00<00:00, 2.60MB/s]\n","Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 8.71MB/s]\n","Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 31.3MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.66MB/s]\n","Downloading model.safetensors: 100% 1.42G/1.42G [00:07<00:00, 184MB/s]\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msebastianobarresi28\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DNLP_project_2023/wandb/run-20230916_200724-kin041vh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeach-brook-1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-roberta-large\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-roberta-large/runs/kin041vh\u001b[0m\n","Test preds frequency: {'H': 140, 'C': 127, 'D': 121, 'G': 114, 'B': 112, 'E': 110, 'F': 101, 'A': 101}\n","Epoch 1: Loss: Train 0.3863; Val 0.3717\n","Classification Acc: Train 0.8746; Val 0.875\n","Classification Macro F1: Train 0.4669; Val 0.4667\n","Instance Acc: Val 0.2199, Test 0.2505\n","Test preds frequency: {'D': 127, 'H': 123, 'F': 120, 'C': 119, 'A': 115, 'B': 114, 'G': 114, 'E': 94}\n","Epoch 2: Loss: Train 0.3707; Val 0.3532\n","Classification Acc: Train 0.875; Val 0.8744\n","Classification Macro F1: Train 0.4667; Val 0.4677\n","Instance Acc: Val 0.3329, Test 0.4654\n","Test preds frequency: {'B': 122, 'H': 122, 'A': 121, 'G': 118, 'C': 118, 'F': 117, 'D': 109, 'E': 99}\n","Epoch 3: Loss: Train 0.3498; Val 0.3383\n","Classification Acc: Train 0.8749; Val 0.8724\n","Classification Macro F1: Train 0.4783; Val 0.6083\n","Instance Acc: Val 0.4177, Test 0.5475\n","Test preds frequency: {'H': 126, 'D': 121, 'G': 118, 'B': 117, 'C': 117, 'F': 115, 'A': 115, 'E': 97}\n","Epoch 4: Loss: Train 0.3287; Val 0.328\n","Classification Acc: Train 0.8781; Val 0.8664\n","Classification Macro F1: Train 0.5389; Val 0.6628\n","Instance Acc: Val 0.4656, Test 0.5551\n","Test preds frequency: {'F': 126, 'H': 125, 'C': 123, 'B': 121, 'D': 115, 'G': 114, 'A': 105, 'E': 97}\n","Epoch 5: Loss: Train 0.3094; Val 0.3096\n","Classification Acc: Train 0.882; Val 0.8765\n","Classification Macro F1: Train 0.5974; Val 0.6725\n","Instance Acc: Val 0.5061, Test 0.5551\n","--- Execution time : 1717.6209769248962 seconds ---\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▁▁▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▇▅▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▇▇▅▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁▄▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▆▄▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.882\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.3094\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8765\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.5061\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.3096\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpeach-brook-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-roberta-large/runs/kin041vh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-roberta-large/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk4MjAyOTcy/version_details/v0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230916_200724-kin041vh/logs\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### Roberta Base\n"],"metadata":{"id":"7yjpkSdCX7wv"}},{"cell_type":"code","source":["!python train_qasc.py --name \"roberta-base\" --epochs 5 --lr 1e-6 --bs 40 --eval-bs 40 --savings_path /content/drive/MyDrive/Project/Saves"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qb9DRzn6X-uo","executionInfo":{"status":"ok","timestamp":1694892249897,"user_tz":-120,"elapsed":638426,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}},"outputId":"236dbe99-53d7-4bc8-acab-1a3fdd5aa50e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-09-16 19:13:34.175279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Namespace(lr=1e-06, wd=0.0, warm_up_steps=0, adam_epsilon=1e-08, bs=40, eval_bs=40, epochs=5, name='roberta-base', shuffle=False, input_format='1', savings_path='/content/drive/MyDrive/Project/Saves', seed=0)\n","Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 3.18MB/s]\n","Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 13.3MB/s]\n","Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 85.1MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 16.5MB/s]\n","Downloading model.safetensors: 100% 499M/499M [00:01<00:00, 322MB/s]\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DNLP_project_2023/wandb/run-20230916_191409-nkk1lbj2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeachy-wind-4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-roberta-base\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-roberta-base/runs/nkk1lbj2\u001b[0m\n","Test preds frequency: {'D': 131, 'C': 123, 'E': 118, 'B': 117, 'G': 117, 'H': 110, 'F': 106, 'A': 104}\n","Epoch 1: Loss: Train 0.3901; Val 0.3774\n","Classification Acc: Train 0.8672; Val 0.875\n","Classification Macro F1: Train 0.4757; Val 0.4667\n","Instance Acc: Val 0.156, Test 0.1415\n","Test preds frequency: {'B': 128, 'H': 125, 'G': 121, 'C': 118, 'D': 112, 'A': 112, 'F': 109, 'E': 101}\n","Epoch 2: Loss: Train 0.3732; Val 0.3657\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.242, Test 0.2138\n","Test preds frequency: {'G': 127, 'F': 121, 'D': 120, 'B': 120, 'H': 118, 'C': 117, 'A': 111, 'E': 92}\n","Epoch 3: Loss: Train 0.3655; Val 0.3571\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.274, Test 0.2581\n","Test preds frequency: {'H': 128, 'G': 125, 'D': 119, 'C': 118, 'B': 118, 'F': 113, 'A': 107, 'E': 98}\n","Epoch 4: Loss: Train 0.3585; Val 0.3513\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.3133, Test 0.2505\n","Test preds frequency: {'H': 131, 'G': 128, 'C': 123, 'D': 117, 'F': 116, 'B': 115, 'A': 101, 'E': 95}\n","Epoch 5: Loss: Train 0.3501; Val 0.3451\n","Classification Acc: Train 0.8751; Val 0.8752\n","Classification Macro F1: Train 0.469; Val 0.4787\n","Instance Acc: Val 0.3354, Test 0.2462\n","--- Execution time : 625.050618648529 seconds ---\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▅▄▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁▁▁▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁▄▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▅▄▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.8751\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.3501\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8752\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.3354\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.3451\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpeachy-wind-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-roberta-base/runs/nkk1lbj2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-roberta-base/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0MDYzNjY4/version_details/v3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230916_191409-nkk1lbj2/logs\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### DeBerta Base"],"metadata":{"id":"U-yIUgACNEXc"}},{"cell_type":"code","source":["!python train_qasc.py --name \"microsoft/deberta-v3-base\" --epochs 5 --lr 1e-6 --bs 40 --eval-bs 40 --savings_path /content/drive/MyDrive/Project/Saves"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKeGfXKlC8-g","executionInfo":{"status":"ok","timestamp":1694893144235,"user_tz":-120,"elapsed":894357,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}},"outputId":"ca7482bb-60a7-45a5-fb02-a3e99cc79061"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-09-16 19:24:13.076152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Namespace(lr=1e-06, wd=0.0, warm_up_steps=0, adam_epsilon=1e-08, bs=40, eval_bs=40, epochs=5, name='microsoft/deberta-v3-base', shuffle=False, input_format='1', savings_path='/content/drive/MyDrive/Project/Saves', seed=0)\n","Downloading (…)okenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 229kB/s]\n","Downloading (…)lve/main/config.json: 100% 579/579 [00:00<00:00, 3.99MB/s]\n","Downloading spm.model: 100% 2.46M/2.46M [00:00<00:00, 50.5MB/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Downloading pytorch_model.bin: 100% 371M/371M [00:01<00:00, 209MB/s]\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msebastianobarresi28\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DNLP_project_2023/wandb/run-20230916_192424-ylsokrrd\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mleafy-lion-2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-deberta-v3-base\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-deberta-v3-base/runs/ylsokrrd\u001b[0m\n","Test preds frequency: {'C': 140, 'A': 125, 'G': 118, 'F': 112, 'H': 111, 'D': 107, 'B': 107, 'E': 106}\n","Epoch 1: Loss: Train 0.3889; Val 0.3381\n","Classification Acc: Train 0.8614; Val 0.875\n","Classification Macro F1: Train 0.4779; Val 0.4667\n","Instance Acc: Val 0.4005, Test 0.5659\n","Test preds frequency: {'C': 138, 'A': 126, 'G': 119, 'D': 112, 'H': 112, 'E': 109, 'F': 105, 'B': 105}\n","Epoch 2: Loss: Train 0.3393; Val 0.3242\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4669; Val 0.4667\n","Instance Acc: Val 0.4484, Test 0.608\n","Test preds frequency: {'C': 134, 'G': 124, 'A': 123, 'H': 116, 'E': 112, 'B': 109, 'D': 106, 'F': 102}\n","Epoch 3: Loss: Train 0.323; Val 0.3168\n","Classification Acc: Train 0.8754; Val 0.8788\n","Classification Macro F1: Train 0.4735; Val 0.5344\n","Instance Acc: Val 0.4902, Test 0.6285\n","Test preds frequency: {'C': 139, 'G': 123, 'A': 120, 'H': 117, 'B': 111, 'F': 107, 'D': 105, 'E': 104}\n","Epoch 4: Loss: Train 0.3094; Val 0.3095\n","Classification Acc: Train 0.8798; Val 0.8856\n","Classification Macro F1: Train 0.5305; Val 0.6292\n","Instance Acc: Val 0.5184, Test 0.6317\n","Test preds frequency: {'C': 137, 'A': 116, 'G': 115, 'D': 114, 'B': 114, 'H': 114, 'F': 113, 'E': 103}\n","Epoch 5: Loss: Train 0.2979; Val 0.303\n","Classification Acc: Train 0.8832; Val 0.8842\n","Classification Macro F1: Train 0.5893; Val 0.6645\n","Instance Acc: Val 0.5356, Test 0.6188\n","--- Execution time : 879.3598494529724 seconds ---\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁▅▅▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▄▃▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁▁▄█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁▃▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▅▄▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.8832\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.2979\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8842\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.5356\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.303\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mleafy-lion-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-deberta-v3-base/runs/ylsokrrd\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-deberta-v3-base/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0MDUyMjU1/version_details/v1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230916_192424-ylsokrrd/logs\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### DistilBERT Base"],"metadata":{"id":"GeddVGAXOjwE"}},{"cell_type":"code","source":["!python train_qasc.py --name \"distilbert-base-uncased\" --epochs 5 --lr 1e-6 --bs 40 --eval-bs 40 --savings_path /content/drive/MyDrive/Project/Saves"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4v8XyFffOoq3","executionInfo":{"status":"ok","timestamp":1694893472825,"user_tz":-120,"elapsed":328602,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}},"outputId":"989e5f56-63ec-450e-a8fb-4c4916cc580f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-09-16 19:39:07.402491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Namespace(lr=1e-06, wd=0.0, warm_up_steps=0, adam_epsilon=1e-08, bs=40, eval_bs=40, epochs=5, name='distilbert-base-uncased', shuffle=False, input_format='1', savings_path='/content/drive/MyDrive/Project/Saves', seed=0)\n","Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 149kB/s]\n","Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 4.15MB/s]\n","Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 5.18MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 25.7MB/s]\n","Downloading model.safetensors: 100% 268M/268M [00:01<00:00, 184MB/s]\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msebastianobarresi28\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DNLP_project_2023/wandb/run-20230916_193916-jtsvcuyn\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mavid-sponge-2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-distilbert-base-uncased\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-distilbert-base-uncased/runs/jtsvcuyn\u001b[0m\n","Test preds frequency: {'D': 124, 'E': 122, 'F': 121, 'A': 117, 'C': 116, 'G': 112, 'H': 107, 'B': 107}\n","Epoch 1: Loss: Train 0.392; Val 0.3753\n","Classification Acc: Train 0.8692; Val 0.875\n","Classification Macro F1: Train 0.4732; Val 0.4667\n","Instance Acc: Val 0.1806, Test 0.1609\n","Test preds frequency: {'D': 125, 'B': 121, 'C': 119, 'F': 117, 'G': 117, 'A': 114, 'E': 111, 'H': 102}\n","Epoch 2: Loss: Train 0.374; Val 0.3694\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.2174, Test 0.1814\n","Test preds frequency: {'G': 125, 'A': 121, 'H': 116, 'D': 115, 'F': 113, 'E': 113, 'B': 113, 'C': 110}\n","Epoch 3: Loss: Train 0.3676; Val 0.3621\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.2432, Test 0.1944\n","Test preds frequency: {'G': 131, 'A': 126, 'B': 126, 'C': 122, 'D': 111, 'F': 106, 'H': 104, 'E': 100}\n","Epoch 4: Loss: Train 0.3596; Val 0.3561\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.274, Test 0.2073\n","Test preds frequency: {'G': 136, 'C': 128, 'A': 127, 'B': 119, 'H': 106, 'E': 104, 'D': 104, 'F': 102}\n","Epoch 5: Loss: Train 0.3521; Val 0.3515\n","Classification Acc: Train 0.8751; Val 0.8752\n","Classification Macro F1: Train 0.4697; Val 0.4751\n","Instance Acc: Val 0.2973, Test 0.2084\n","--- Execution time : 312.0665240287781 seconds ---\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▅▄▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁▁▁▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁▃▅▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▆▄▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.8751\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.3521\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.8752\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.2973\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.3515\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mavid-sponge-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-distilbert-base-uncased/runs/jtsvcuyn\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-distilbert-base-uncased/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0MDU0ODAx/version_details/v1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230916_193916-jtsvcuyn/logs\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### T5 Base\n"],"metadata":{"id":"xAskxbPbPHZA"}},{"cell_type":"code","source":["!python train_qasc.py --name \"T5-base\" --epochs 5 --lr 1e-6 --bs 40 --eval-bs 40 --savings_path /content/drive/MyDrive/Project/Saves"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTxu4w-TPK2h","executionInfo":{"status":"ok","timestamp":1694894823183,"user_tz":-120,"elapsed":1350372,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}},"outputId":"282a0dfa-a3ea-49d8-a749-0cbd5c62abe2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-09-16 19:44:35.864901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Namespace(lr=1e-06, wd=0.0, warm_up_steps=0, adam_epsilon=1e-08, bs=40, eval_bs=40, epochs=5, name='T5-base', shuffle=False, input_format='1', savings_path='/content/drive/MyDrive/Project/Saves', seed=0)\n","Downloading (…)lve/main/config.json: 100% 1.21k/1.21k [00:00<00:00, 5.42MB/s]\n","Downloading (…)ve/main/spiece.model: 100% 792k/792k [00:00<00:00, 3.14MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 4.27MB/s]\n","Downloading model.safetensors: 100% 892M/892M [00:03<00:00, 260MB/s]\n","Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at T5-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using sep_token, but it is not set yet.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msebastianobarresi28\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DNLP_project_2023/wandb/run-20230916_194449-a57x8vx7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msolar-salad-2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-T5-base\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-T5-base/runs/a57x8vx7\u001b[0m\n","Test preds frequency: {'H': 144, 'D': 125, 'C': 123, 'F': 123, 'G': 109, 'E': 108, 'A': 99, 'B': 95}\n","Epoch 1: Loss: Train 0.4489; Val 0.3806\n","Classification Acc: Train 0.832; Val 0.875\n","Classification Macro F1: Train 0.4918; Val 0.4667\n","Instance Acc: Val 0.1093, Test 0.1102\n","Test preds frequency: {'H': 149, 'F': 122, 'D': 120, 'C': 118, 'A': 109, 'E': 109, 'G': 108, 'B': 91}\n","Epoch 2: Loss: Train 0.3799; Val 0.3783\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.129, Test 0.1263\n","Test preds frequency: {'H': 141, 'C': 124, 'F': 122, 'D': 122, 'A': 114, 'E': 106, 'G': 105, 'B': 92}\n","Epoch 3: Loss: Train 0.379; Val 0.3767\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.1474, Test 0.1371\n","Test preds frequency: {'H': 138, 'D': 123, 'C': 123, 'F': 118, 'A': 114, 'G': 111, 'E': 108, 'B': 91}\n","Epoch 4: Loss: Train 0.377; Val 0.3752\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.1634, Test 0.1566\n","Test preds frequency: {'H': 127, 'C': 127, 'F': 122, 'A': 121, 'D': 120, 'G': 107, 'E': 106, 'B': 96}\n","Epoch 5: Loss: Train 0.3759; Val 0.374\n","Classification Acc: Train 0.875; Val 0.875\n","Classification Macro F1: Train 0.4667; Val 0.4667\n","Instance Acc: Val 0.1916, Test 0.1577\n","--- Execution time : 1335.04514503479 seconds ---\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy ▁████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss █▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy ▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy ▁▃▄▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss █▆▄▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    Train CLS Accuracy 0.875\n","\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.3759\n","\u001b[34m\u001b[1mwandb\u001b[0m:      Val CLS Accuracy 0.875\n","\u001b[34m\u001b[1mwandb\u001b[0m: Val Instance Accuracy 0.1916\n","\u001b[34m\u001b[1mwandb\u001b[0m:              Val Loss 0.374\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msolar-salad-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-T5-base/runs/a57x8vx7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/sebastianobarresi28/QASC-T5-base/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0MDYwNzE2/version_details/v1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230916_194449-a57x8vx7/logs\u001b[0m\n"]}]},{"cell_type":"markdown","source":["## TEAM with GenMC"],"metadata":{"id":"lCl_zNHGPZfl"}},{"cell_type":"code","source":["!python run_genmc_team.py --model_path t5-base --output_dir /content/drive/MyDrive/Project/Saves --data_path_train ./data/qasc/in_hourse/train.jsonl  --data_path_dev ./data/qasc/in_hourse/dev.jsonl  --data_path_test ./data/qasc/in_hourse/test.jsonl"],"metadata":{"id":"VnfW8ylQRwwq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"496e217a-6e1f-48d7-e42c-466cb90d94ed","executionInfo":{"status":"ok","timestamp":1693652546234,"user_tz":-120,"elapsed":7435842,"user":{"displayName":"Sebastiano Barresi","userId":"08032471211001708852"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n","{\n","  \"lr\": 5e-05,\n","  \"model\": \"t5-base\",\n","  \"seed\": 1,\n","  \"bs\": 64,\n","  \"gradient_accumulation_steps\": 4,\n","  \"epoch\": 30,\n","  \"train_path\": \"./data/qasc/in_hourse/train.jsonl\",\n","  \"dev_path\": \"./data/qasc/in_hourse/dev.jsonl\",\n","  \"test_path\": \"./data/qasc/in_hourse/test.jsonl\",\n","  \"train_size\": 7320,\n","  \"dev_size\": 814,\n","  \"test_size\": 926,\n","  \"num_hidden_layers\": 1,\n","  \"external_sent_num\": null,\n","  \"alpha\": 1,\n","  \"beta\": 1\n","}\n","Downloading (…)ve/main/spiece.model: 100% 792k/792k [00:00<00:00, 3.03MB/s]\n","Downloading (…)lve/main/config.json: 100% 1.21k/1.21k [00:00<00:00, 5.31MB/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:217: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Downloading model.safetensors: 100% 892M/892M [00:02<00:00, 364MB/s]\n","Downloading (…)neration_config.json: 100% 147/147 [00:00<00:00, 794kB/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1592: FutureWarning: `T5ForConditionalGeneration.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance {'encoder.block.0': 0, 'encoder.block.1': 1, ...}\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:930: FutureWarning: `T5Stack.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance {'block.0': 0, 'block.1': 1, ...}\n","  warnings.warn(\n","  0% 0/102 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","2023-09-02 08:58:44.714437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","  1% 1/102 [00:02<04:49,  2.87s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","100% 102/102 [00:42<00:00,  2.42it/s]\n","best_dev_acc: 0.128992628992629\n","100% 458/458 [04:03<00:00,  1.88it/s,  Epoch:0 loss:4.5598]\n","100% 102/102 [00:23<00:00,  4.39it/s]\n","dev_acc: 0.128992628992629\n","100% 458/458 [03:42<00:00,  2.05it/s,  Epoch:1 loss:4.147]\n","100% 102/102 [00:20<00:00,  4.95it/s]\n","dev_acc: 0.128992628992629\n","100% 458/458 [03:39<00:00,  2.09it/s,  Epoch:2 loss:3.9366]\n","100% 102/102 [00:22<00:00,  4.63it/s]\n","dev_acc: 0.128992628992629\n","100% 458/458 [03:38<00:00,  2.09it/s,  Epoch:3 loss:3.7952]\n","100% 102/102 [00:20<00:00,  4.94it/s]\n","dev_acc: 0.128992628992629\n","100% 458/458 [03:35<00:00,  2.12it/s,  Epoch:4 loss:3.682]\n","100% 102/102 [00:19<00:00,  5.17it/s]\n","dev_acc: 0.128992628992629\n","100% 458/458 [03:36<00:00,  2.12it/s,  Epoch:5 loss:3.5869]\n","100% 102/102 [00:18<00:00,  5.41it/s]\n","dev_acc: 0.128992628992629\n","100% 458/458 [03:32<00:00,  2.15it/s,  Epoch:6 loss:3.5061]\n","100% 102/102 [00:18<00:00,  5.61it/s]\n","dev_acc: 0.13882063882063883\n","100% 116/116 [00:20<00:00,  5.69it/s]\n","new best dev acc: 0.13882063882063883 test_acc: 0.15442764578833693 rouge: 0.36554337522544267\n","100% 458/458 [03:35<00:00,  2.13it/s,  Epoch:7 loss:3.432]\n","100% 102/102 [00:18<00:00,  5.64it/s]\n","dev_acc: 0.1511056511056511\n","100% 116/116 [00:20<00:00,  5.69it/s]\n","new best dev acc: 0.1511056511056511 test_acc: 0.1717062634989201 rouge: 0.36333968710233655\n","100% 458/458 [03:33<00:00,  2.14it/s,  Epoch:8 loss:3.3624]\n","100% 102/102 [00:17<00:00,  5.70it/s]\n","dev_acc: 0.14864864864864866\n","100% 458/458 [03:31<00:00,  2.16it/s,  Epoch:9 loss:3.2981]\n","100% 102/102 [00:17<00:00,  5.76it/s]\n","dev_acc: 0.15601965601965603\n","100% 116/116 [00:20<00:00,  5.58it/s]\n","new best dev acc: 0.15601965601965603 test_acc: 0.18466522678185746 rouge: 0.36771498338551933\n","100% 458/458 [03:33<00:00,  2.14it/s,  Epoch:10 loss:3.2376]\n","100% 102/102 [00:17<00:00,  5.95it/s]\n","dev_acc: 0.16584766584766586\n","100% 116/116 [00:20<00:00,  5.79it/s]\n","new best dev acc: 0.16584766584766586 test_acc: 0.19222462203023757 rouge: 0.36463980748924746\n","100% 458/458 [03:30<00:00,  2.17it/s,  Epoch:11 loss:3.1805]\n","100% 102/102 [00:16<00:00,  6.03it/s]\n","dev_acc: 0.19164619164619165\n","100% 116/116 [00:20<00:00,  5.70it/s]\n","new best dev acc: 0.19164619164619165 test_acc: 0.2203023758099352 rouge: 0.37479806585075043\n","100% 458/458 [03:31<00:00,  2.17it/s,  Epoch:12 loss:3.1268]\n","100% 102/102 [00:16<00:00,  6.24it/s]\n","dev_acc: 0.20515970515970516\n","100% 116/116 [00:19<00:00,  5.96it/s]\n","new best dev acc: 0.20515970515970516 test_acc: 0.2267818574514039 rouge: 0.37182950612865023\n","100% 458/458 [03:26<00:00,  2.22it/s,  Epoch:13 loss:3.0747]\n","100% 102/102 [00:17<00:00,  5.92it/s]\n","dev_acc: 0.2936117936117936\n","100% 116/116 [00:19<00:00,  5.96it/s]\n","new best dev acc: 0.2936117936117936 test_acc: 0.2991360691144708 rouge: 0.36696654517759086\n","100% 458/458 [03:28<00:00,  2.20it/s,  Epoch:14 loss:3.0247]\n","100% 102/102 [00:15<00:00,  6.43it/s]\n","dev_acc: 0.29975429975429974\n","100% 116/116 [00:19<00:00,  5.97it/s]\n","new best dev acc: 0.29975429975429974 test_acc: 0.3142548596112311 rouge: 0.3790235071314411\n","100% 458/458 [03:28<00:00,  2.20it/s,  Epoch:15 loss:2.9765]\n","100% 102/102 [00:16<00:00,  6.35it/s]\n","dev_acc: 0.3083538083538084\n","100% 116/116 [00:19<00:00,  6.03it/s]\n","new best dev acc: 0.3083538083538084 test_acc: 0.3045356371490281 rouge: 0.3811497076367111\n","100% 458/458 [03:26<00:00,  2.22it/s,  Epoch:16 loss:2.9298]\n","100% 102/102 [00:15<00:00,  6.38it/s]\n","dev_acc: 0.32678132678132676\n","100% 116/116 [00:18<00:00,  6.19it/s]\n","new best dev acc: 0.32678132678132676 test_acc: 0.32937365010799136 rouge: 0.3753515373333083\n","100% 458/458 [03:25<00:00,  2.23it/s,  Epoch:17 loss:2.8844]\n","100% 102/102 [00:17<00:00,  5.97it/s]\n","dev_acc: 0.2972972972972973\n","100% 458/458 [03:24<00:00,  2.24it/s,  Epoch:18 loss:2.8402]\n","100% 102/102 [00:16<00:00,  6.30it/s]\n","dev_acc: 0.32923832923832924\n","100% 116/116 [00:18<00:00,  6.16it/s]\n","new best dev acc: 0.32923832923832924 test_acc: 0.3282937365010799 rouge: 0.37390524471349745\n","100% 458/458 [03:26<00:00,  2.22it/s,  Epoch:19 loss:2.7974]\n","100% 102/102 [00:15<00:00,  6.46it/s]\n","dev_acc: 0.32678132678132676\n","100% 458/458 [03:24<00:00,  2.24it/s,  Epoch:20 loss:2.7561]\n","100% 102/102 [00:15<00:00,  6.51it/s]\n","dev_acc: 0.3746928746928747\n","100% 116/116 [00:18<00:00,  6.31it/s]\n","new best dev acc: 0.3746928746928747 test_acc: 0.33369330453563717 rouge: 0.38098809095324526\n","100% 458/458 [03:25<00:00,  2.23it/s,  Epoch:21 loss:2.7158]\n","100% 102/102 [00:15<00:00,  6.43it/s]\n","dev_acc: 0.3783783783783784\n","100% 116/116 [00:18<00:00,  6.22it/s]\n","new best dev acc: 0.3783783783783784 test_acc: 0.33801295896328293 rouge: 0.3814535622607788\n","100% 458/458 [03:25<00:00,  2.23it/s,  Epoch:22 loss:2.6762]\n","100% 102/102 [00:16<00:00,  6.34it/s]\n","dev_acc: 0.3796068796068796\n","100% 116/116 [00:18<00:00,  6.13it/s]\n","new best dev acc: 0.3796068796068796 test_acc: 0.33153347732181426 rouge: 0.3819690145150896\n","100% 458/458 [03:25<00:00,  2.23it/s,  Epoch:23 loss:2.6377]\n","100% 102/102 [00:15<00:00,  6.51it/s]\n","dev_acc: 0.386977886977887\n","100% 116/116 [00:17<00:00,  6.51it/s]\n","new best dev acc: 0.386977886977887 test_acc: 0.37365010799136067 rouge: 0.3980691411124897\n","100% 458/458 [03:27<00:00,  2.21it/s,  Epoch:24 loss:2.6001]\n","100% 102/102 [00:15<00:00,  6.38it/s]\n","dev_acc: 0.371007371007371\n","100% 458/458 [03:22<00:00,  2.26it/s,  Epoch:25 loss:2.5634]\n","100% 102/102 [00:15<00:00,  6.52it/s]\n","dev_acc: 0.3931203931203931\n","100% 116/116 [00:18<00:00,  6.32it/s]\n","new best dev acc: 0.3931203931203931 test_acc: 0.35853131749460043 rouge: 0.38782356032383003\n","100% 458/458 [03:26<00:00,  2.21it/s,  Epoch:26 loss:2.5276]\n","100% 102/102 [00:15<00:00,  6.47it/s]\n","dev_acc: 0.414004914004914\n","100% 116/116 [00:18<00:00,  6.28it/s]\n","new best dev acc: 0.414004914004914 test_acc: 0.3596112311015119 rouge: 0.3900732944859229\n","100% 458/458 [03:26<00:00,  2.22it/s,  Epoch:27 loss:2.4926]\n","100% 102/102 [00:16<00:00,  6.37it/s]\n","dev_acc: 0.4004914004914005\n","100% 458/458 [03:23<00:00,  2.25it/s,  Epoch:28 loss:2.4582]\n","100% 102/102 [00:15<00:00,  6.58it/s]\n","dev_acc: 0.4176904176904177\n","100% 116/116 [00:18<00:00,  6.33it/s]\n","new best dev acc: 0.4176904176904177 test_acc: 0.34773218142548595 rouge: 0.39425299882304754\n","100% 458/458 [03:26<00:00,  2.22it/s,  Epoch:29 loss:2.4242]\n","100% 102/102 [00:15<00:00,  6.42it/s]\n","dev_acc: 0.4189189189189189\n","100% 116/116 [00:17<00:00,  6.47it/s]\n","new best dev acc: 0.4189189189189189 test_acc: 0.357451403887689 rouge: 0.39829164834605024\n","best dev acc: 0.4189189189189189 best_test_acc: 0.357451403887689 best_dev_rouge_score: 0.39829164834605024 best_test_rouge_score: 0.36806193284749367\n"]}]}]}